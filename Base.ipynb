{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiArmBandit:\n",
    "    def __init__(self, steps = 10000, k = 10,\\\n",
    "                mu = 0, sigma = 1, epsilon = 0.01, \\\n",
    "                stationary = True, alpha = None,\\\n",
    "                init_q = 0, upper_bound = False):\n",
    "        self.steps = steps\n",
    "        self.k = k\n",
    "        self.epsilon = epsilon\n",
    "        self.mu = 0\n",
    "        self.sigma = 1\n",
    "        self.stationary = stationary\n",
    "        self.alpha = alpha\n",
    "        self.init_q = init_q\n",
    "        self.upper_bound = upper_bound\n",
    "        \n",
    "        self.reset_values()\n",
    "                \n",
    "        self.overall_average_rewards =  np.zeros(steps)\n",
    "        self.overall_percentage_optimal = np.zeros(steps)\n",
    "        \n",
    "    def reset_values(self):\n",
    "        if self.stationary:\n",
    "            self.q_star = np.random.normal(self.mu, self.sigma, self.k)        \n",
    "        else:\n",
    "            init_value = np.random.normal(self.mu, self.sigma)\n",
    "            self.q_star = np.ones(self.k) * init_value\n",
    "        \n",
    "        self.estimates = np.ones(self.k) * self.init_q\n",
    "        self.counts = np.zeros(self.k)\n",
    "        \n",
    "        self.total_reward = 0\n",
    "        self.average_rewards = np.zeros(self.steps)\n",
    "        \n",
    "        self.optimal_count = 0\n",
    "        self.percentage_optimal = np.zeros(self.steps)\n",
    "    \n",
    "    # Returns index of action selected\n",
    "    def select_action(self, step):\n",
    "        \n",
    "        if not self.stationary:\n",
    "            #Update Q Values With Random Walk\n",
    "            self.q_star += np.random.normal(0,0.01,self.k)\n",
    "        \n",
    "        prob = random.random()\n",
    "        selected_action = None\n",
    "        \n",
    "        if self.upper_bound:\n",
    "            temp = (( np.log(step + 1.0)/self.counts)**0.5) * 2 + self.estimates\n",
    "            selected_action = np.argmax(temp)\n",
    "        else:\n",
    "            if prob < self.epsilon:\n",
    "                selected_action =  random.randint(0,self.k - 1)\n",
    "            else:\n",
    "                selected_action = np.argmax(self.estimates)\n",
    "        \n",
    "        optimal_action = np.argmax(self.q_star)\n",
    "        if selected_action == optimal_action:\n",
    "            self.optimal_count += 1\n",
    "        \n",
    "        return selected_action\n",
    "    \n",
    "    #Performs an independent move\n",
    "    def make_move(self, step):\n",
    "        action = self.select_action(step)\n",
    "        q_star_action = self.q_star[action]\n",
    "        variance = 1\n",
    "        \n",
    "        #Get reward from normal with mean q*(a) and vairance 1\n",
    "        reward = np.random.normal(q_star_action, variance)\n",
    "        \n",
    "        # Update Step\n",
    "        old_count = self.counts[action]\n",
    "        old_estimate = self.estimates[action]\n",
    "        \n",
    "        new_count = old_count + 1\n",
    "        if not self.alpha:\n",
    "            new_estimate = old_estimate + (reward - old_estimate)*1.0/new_count\n",
    "        else:\n",
    "            new_estimate = old_estimate + (reward - old_estimate) * self.alpha\n",
    "            \n",
    "        self.counts[action] = new_count\n",
    "        self.estimates[action] = new_estimate\n",
    "        \n",
    "        self.total_reward += reward\n",
    "        average_reward = self.total_reward/(step + 1)\n",
    "        self.average_rewards[step] = average_reward\n",
    "        \n",
    "        self.percentage_optimal[step] = self.optimal_count/(step + 1)\n",
    "\n",
    "    def play_game_once(self):\n",
    "        for step in range(self.steps):\n",
    "            self.make_move(step)\n",
    "    \n",
    "    def play(self, runs = 1000):\n",
    "        for run in tqdm(range(runs)):\n",
    "            self.play_game_once()\n",
    "            self.overall_average_rewards += self.average_rewards\n",
    "            self.overall_percentage_optimal += self.percentage_optimal \n",
    "            self.reset_values()\n",
    "        self.overall_average_rewards = self.overall_average_rewards/runs\n",
    "        self.overall_percentage_optimal = self.overall_percentage_optimal/runs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
